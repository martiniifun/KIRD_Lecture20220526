{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 딥러닝으로 다이빙하기 전에"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1-1. 우리가 지나쳤던 머신러닝 알고리즘들"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1-1-1. 의사결정나무 : \"올해 휴가는 어디로 갈까?\" since 1984, Leo Breiman\n",
    "\n",
    "\"이번 여름휴가는 바다로 갈까, 산으로 갈까?\"\n",
    "\n",
    "여름 휴가철이 되면 가족들과 함께 어디로 휴가 갈지 매번 고민들 하시죠?\n",
    "막연하긴 하지만, \"엄청 더우면(체감온도 30도 이상) 바다로 가고, 그렇지 않으면 산으로 간다.\"라고\n",
    "마음속으로 나름 확고한 알고리즘을 짜 둔 분이라면 큰 고민 없이 기준에 맞춰 의사결정을 하실 겁니다.\n",
    "\n",
    "의사결정트리 알고리즘은 이런 방식으로 특정 조건을 수립(조정)하고\n",
    "해당 조건에 따라 데이터를 분류하거나 예측합니다.\n",
    "\n",
    "# 휴가지 선정방법(예시)\n",
    "![휴가지선정방법](https://i.ibb.co/N1F6FHS/decisiontree-ex1.png)\n",
    "이처럼 분석과정이 직관적이고 이해하기 쉽다는 점이 의사결정나무의 가장 큰 장점입니다.\n",
    "\n",
    "# 의사결정나무의 장단점\n",
    "이밖에도 의사결정나무의 장단점으로는\n",
    "1. 알고리즘 해석이 용이하며, 분류기준이 되는 중요특성을 찾는 경우에도 유용하게 쓰인다.\n",
    "2. 정수, 실수데이터 뿐만 아니라 범주형데이터(성별 및 불리언) 또한 사용이 가능하다.\n",
    "3. 필요한 전처리가 비교적 적다. 스케일링, 정규화 등의 과정이 필요없어진다.\n",
    "4. 다만, 과적합되기 쉽다. 그래서 훈련데이터를 검토해서 이상값이나 예외값을 정제하는 등의 대책이 필요하다.\n",
    "\n",
    "# 의사결정나무 알고리즘(간략히)\n",
    "\n",
    "의사결정나무 알고리즘은 (다 비슷해 보이지만) 데이터를 분류하는 방법에 따라 제법 여러 가지로 나뉩니다.\n",
    "이 중 의사결정나무 알고리즘의 시초이며, 현재까지도 가장 많이 쓰이는 방법은, \"지니 계수\"와 \"분산 감소량\"을 사용해서 이진분류하는 방법입니다. 지니 계수는 범주형 변수를 분류할 때, 분산 감소량은 연속형 변수를 분류할 때 사용하는 방식입니다.\n",
    "\n",
    "> 지니계수는 원래 경제학에서 경제적 불평등(소득 불균형)을 계수화하는 방법입니다. 경제학자인 코라도지니Corrado Gini의 이름에서 딴 계수로, 완전 평등하다면 0, 완전불평등한 상태라면 1이 됩니다. 이를 머신러닝에 적용할 때에는 그 의미가 재해석되는데, 데이터의 속성들이 많이 일치하면 1이 되고, 반대로 속성들이 제각기 다른 형태이면 0이 됩니다. 우리가 원하는 것은 비슷한 데이터끼리 잘 묶어주는 것이므로, 분류 전후의 지니 계수의 차이가 큰 속성을 기준으로 우선분할합니다. 즉, 데이터들의 속성이 비슷한 것끼리 구분지어 묶이게 됩니다.\n",
    "\n",
    "위 방법을 가리켜 카트(CaRT, Classification and Regression Trees) 알고리즘이라고 줄여 부릅니다. 그 밖에도 지니계수 대신 정보이득(information gain)을 사용하는 ID3알고리즘, 이득비율(gain ration)을 사용하여 이진분류가 아니라 세 가지로 분류할 수 있는 C5.0알고리즘 등이 있습니다. 각 알고리즘이나 용어에 대해 궁금하신 분은 구글링을 해보시면 어마어마하게 많은 관련포스팅이 튀어나온다는 것에 꽤 놀라실 것입니다. 게다가 한글로요.. 공부하기 편한 시대입니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 의사결정나무를 만들어봅시다.\n",
    "\n",
    "## 1. 붓꽃(Iris) 분류\n",
    "\n",
    "붓꽃(Iris)은 재미있는 특징이 있습니다. \"세토사\", \"버시컬러\", \"버지니카\" 세 개의 대표종이 (서로 다른 종임에도) 색깔과 형태가 굉장히 유사하다는 것입니다. 이걸 효과적으로 구분할 수 있는 거의 유일한 방법은 꽃잎의 길이와 너비, 그리고 꽃받침의 길이와 너비를 이용하는 것입니다.\n",
    "\n",
    "![](https://machinelearninghd.com/wp-content/uploads/2021/03/iris-dataset.png)\n",
    "\n",
    "과연 의사결정나무로도 효과적인 분류가 가능할까요?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Iris_dataset_scatterplot.svg/1280px-Iris_dataset_scatterplot.svg.png)\n",
    "where, sepal:꽃받침, petal:꽃잎"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "데이터셋 = datasets.load_iris()\n",
    "print(데이터셋['DESCR'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "특성데이터 = 데이터셋.data\n",
    "타겟데이터 = 데이터셋.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "특성데이터  # 0~3 칼럼(특성)은 각각 sepal length, sepal width, petal length 및 petal width를 뜻합니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "타겟데이터  # 타겟데이터 0~2는 차례대로 \"세토사\", \"버시컬러\", \"버지니카\"입니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "본격적으로 모델을 구현하기 전에, 의사결정나무의 구조를 이미지로 시각화하기 위한 소프트웨어인\n",
    "그래프비즈(Graphviz)를 설치하겠습니다.\n",
    "![](https://i.ibb.co/h24w4V2/image.png)\n",
    "\n",
    "환경변수의 시스템변수 Path에 경로를 추가해야 하는데, 설치과정 중 환경변수에 추가할지 물어봅니다. 현재 사용자(current user)의 환경변수에 추가하겠다고 선택하고 설치를 마치시면 됩니다.\n",
    "\n",
    "# 붓꽃 데이터를 분류하기 위한 의사결정나무 구현 코드"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "꽃잎정보 = 특성데이터[:, 2:]  # 모든 데이터셋(행)을 사용하지만, 꽃잎길이와 꽃잎너비 두 개의 특성만 사용해보겠습니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 의사결정나무 모델의 클래스 생성\n",
    "분류기 = DecisionTreeClassifier(criterion='gini', max_depth=3)  # 지니계수gini 대신에 혼잡도entropy를 사용하기도 함."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier(max_depth=3)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 학습\n",
    "분류기.fit(꽃잎정보, 타겟데이터)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# 학습이 완료된 결정나무 형태를 출력\n",
    "with open(\"./iris_dtree.dot\", 'w') as f:\n",
    "    tree.export_graphviz(분류기, out_file=f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "생성된 iris_dtree.dot 파일은\n",
    "익숙해지기 전까지는 알아보기 좀 어렵습니다.\n",
    "이렇게 생긴 텍스트 데이터입니다.\n",
    "\n",
    "```\n",
    "digraph Tree {\n",
    "node [shape=box] ;\n",
    "0 [label=\"X[1] <= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]\"] ;\n",
    "1 [label=\"gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]\"] ;\n",
    "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
    "2 [label=\"X[1] <= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]\"] ;\n",
    "0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
    "3 [label=\"X[0] <= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]\"] ;\n",
    "2 -> 3 ;\n",
    "4 [label=\"gini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]\"] ;\n",
    "3 -> 4 ;\n",
    "5 [label=\"gini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]\"] ;\n",
    "3 -> 5 ;\n",
    "6 [label=\"X[0] <= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]\"] ;\n",
    "2 -> 6 ;\n",
    "7 [label=\"gini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]\"] ;\n",
    "6 -> 7 ;\n",
    "8 [label=\"gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]\"] ;\n",
    "6 -> 8 ;\n",
    "}\n",
    "```\n",
    "\n",
    "그래프비즈는 위의 의사결정트리 모델정보를 간단하게 시각화해줍니다.\n",
    "```dot -T png iris_dtree.dot -o iris_dtree.png```\n",
    "\n",
    "결과는 아래와 같습니다.\n",
    "<img src=\"https://i.ibb.co/pb7rspW/iris-dtree.png\" width=1900>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "이제 해석을 해 볼까요?\n",
    "(화살표는 왼쪽이 True, 오른쪽이 False입니다.)\n",
    "가장 먼저, 꽃잎너비([X1])가 0.8 이하냐 아니냐를 기준으로 나누고, 이하인 것들은 전부 세토사로 분류되었습니다.\n",
    "그렇지 않은 것들은 다시 꽃잎너비(X[1])가 1.75 이하이면서 꽃잎길이([X0])가 4.95 이하이면 대부분 버시컬러로 분류됩니다.\n",
    "반대로 꽃잎너비가 1.75 보다 크면서 꽃잎길이가 4.85 보다 크면 대부분 버지니카로 분류됩니다.\n",
    "\n",
    "일반적으로 다른 머신러닝 알고리즘은 \"블랙박스\"처럼 알고리즘 내부의 동작 원리가 복잡하고 해석하기 난해한데, 의사결정나무는 이처럼 알고리즘 자체가 직관적이고 시각적으로 표현 가능해서 해석하기가 용이하다는 강력한 장점이 있습니다. 마치 스무고개 놀이를 하는 것 같네요."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 유방암 데이터로 악성/양성 종양 분류하기\n",
    "\n",
    "지난 시간에 짧게 실습했던 유방암 관련속성을 보고 악성종양(malignant tumor)을 추정하는 분류기를 하나 더 만들어봅시다. 사이킷런의 위스콘신 유방암 데이터세트를 활용해보겠습니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "from sklearn import datasets, tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# 데이터를 훈련, 테스트 데이터로 나누기\n",
    "cancer = datasets.load_breast_cancer()\n",
    "\n",
    "#     =================   ==============\n",
    "#     Classes                          2\n",
    "#     Samples per class    212(M),357(B)\n",
    "#     Samples total                  569\n",
    "#     Dimensionality                  30\n",
    "#     Features            real, positive\n",
    "#     =================   ==============\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify=cancer.target, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# 의사결정모델 클래스로 인스턴스 생성\n",
    "cancer_model = DecisionTreeClassifier(criterion='entropy', max_depth=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련점수: 0.97 %\n",
      "테스트점수: 0.95 %\n"
     ]
    }
   ],
   "source": [
    "# 모델을 학습\n",
    "cancer_model.fit(x_train, y_train)\n",
    "print(f\"훈련점수: {cancer_model.score(x_train, y_train):0.2f} %\")\n",
    "print(f\"테스트점수: {cancer_model.score(x_test, y_test):0.2f} %\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# 학습한 모델을 dot파일로 출력\n",
    "with open(\"cancer-dtree.dot\", \"w\") as f:\n",
    "    tree.export_graphviz(cancer_model, out_file=f, feature_names=cancer.feature_names, class_names=[\"Malignant\", \"Benign\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "커맨드프롬프트에서 매번 \"dot -T png cancer-dtree.dot -o cancer-dtree.png\"을 실행하기는 다소 번거롭습니다.\n",
    "이에 착안하여, Carlos Jenkins라는 프로그래머의 기여로 pydotplus 모듈이 제작되었습니다.\n",
    "이를 이용하면 파이썬 명령어로 간단히 의사결정트리 모델의 png파일을 생성할 수 있습니다.\n",
    "![](https://warehouse-camo.ingress.cmh1.psfhosted.org/0cbd676d521f041dd0d4fd7eeccde0b9340b6484/68747470733a2f2f7365637572652e67726176617461722e636f6d2f6176617461722f37616139396430656564363032363061323933376535366366393536303133383f73697a653d323235)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydotplus in c:\\users\\smj02\\anaconda3\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.1 in c:\\users\\smj02\\anaconda3\\lib\\site-packages (from pydotplus) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# pydotplus를 설치합니다.\n",
    "!pip install pydotplus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydotplus as pdp\n",
    "graph = pdp.graph_from_dot_file(path=r\"C:\\Users\\smj02\\PycharmProjects\\KIRD_Lecture20220526\\9_딥러닝으로 들어가기 전에\\cancer-dtree.dot\")\n",
    "graph.write_png(\"cancer-dtree.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./cancer-dtree.png\" width=1920>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "간편하게 시각화를 완료했습니다.\n",
    "해석방법은 동일합니다.\n",
    "종양에 대한 특성 중에서 차례대로 worst radius, worst concave points 및 texture error 순서로 영향이 큽니다.\n",
    "Value에 보이는 리스트는 실제 레이블입니다. 추론결과가 아니므로 해석에 유의하여 주시기 바랍니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 의사결정트리 실습을 마치며..\n",
    "\n",
    "의사결정트리는 직관적이고 단순해서 이해하기 쉽습니다. 특히 처리결과를 해석하고 공유하는데 가장 쉬운 것이 큰 장점입니다. 의사결정트리의 또 다른 장점이 하나 있는데, 붓꽃과 유방암 사례처럼 분류기로 사용하는 것이 일반적이지만, 다른 머신러닝 알고리즘을 활용하기 전에 전처리할 목적으로, 사전에 중요한 특성이 무엇인지, 어느 특성이 결과에 가장 민감한 영향을 끼치는지를 찾는 경우에도 간편하고 유용하게 쓰이기도 합니다. 또한 현존하는 머신러닝 모델 중 \"정형화된\" 테이블데이터를 가장 잘 분류하고 예측해줄 수 있는 끝판왕 모델이 바로 의사결정트리로 이루어진 트리 기반 앙상블 모델입니다. 특정 분야에서는 딥러닝을 가볍게 제끼기도 하면서 캐글 경진대회의 대상을 휩쓰는 알고리즘이기도 하고요.\n",
    "\n",
    "흥미가 생긴다면 \"XGBoost와 사이킷런을 활용한 그레이디언트 부스팅(박해선 역)\"이라는 책을 가벼이 한 번 읽어보시는 것도 추천드립니다. (내용이 가벼운 책은 아닙니다ㄷㄷㄷ)\n",
    "![](https://img.danawa.com/images/descFiles/6/51/5050365_VSpecLC7sD_1652588851723.jpeg)\n",
    "\n",
    "그럼 여기서 의사결정트리에 대한 설명을 마칩니다.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 서포트 벡터 머신 알고리즘\n",
    "\n",
    "서포트벡터머신Support Vector Machine은 데이터 분포를 나누는 지도학습의 한 종류입니다. 1963년 러시아 수학자였던 블라디미르 배프닉과 알렉세이 체보넨키스에 의해 선형 서포트 벡터 머신으로 발표되었고, 비교적 최근인 1992년에 블라디비르 배프닉에 의해 다시 한 번 비선형으로 확장되었습니다.\n",
    "\n",
    "서포트벡터머신은 지금까지도 보편적으로 사용하는, 분류를 위한 인기있는, 그리고 아주 강력한 머신러닝 모델입니다. 주로 이진 분류 지도학습에 많이 사용됩니다. 예를 들어 한 나라에 정당이 두 개밖에 없을 때 국민들이 어느 정당을 선택할지 예측하는 이진분류에 사용되는 모델입니다.\n",
    "\n",
    "동작원리나 코드도 굉장히 간단합니다.\n",
    "\n",
    "## 서포트 벡터 머신의 동작 원리\n",
    "\n",
    "서포트벡터머신은 두 단계로 구분작업을 수행합니다.\n",
    "\n",
    "1. 그룹을 나누는 직선을 그린다.\n",
    "2. 더 균형감있게 정가운데로 나누기 위해 직선을 조정한다. 이 때 지지선(support vector)을 이용.\n",
    "\n",
    "![](https://i.ibb.co/0qqgGJp/004.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "이처럼 최적의 경계를 찾기 위해 직선 사이의 거리인 \"마진\"이 최대가 되도록 직선을 조정하여 분류 간에 최대한 분리시키는 것이 서포트 벡터 머신이 추구하는 개념입니다. 보다 구체적인 공식이나 개념은 구글링을 해 보셔도 좋지만, 기본개념만 알아두셔도 충분하다고 생각됩니다.\n",
    "\n",
    "## 서포트 벡터 머신으로 당뇨병 분류하기\n",
    "\n",
    "미국 애리조나 주에 정착해 살던 PIMA 인디언은 애초 당뇨병에 걸린 환자가 거의 없었습니다. 그런데 1990년대 후반부터 서구식 식습관으로 변함에 따라 비만과 당뇨병이 급증했다고 합니다. UCI대학교에서 제공하는 피마 인디언의 당뇨병 데이터세트를 통해 서포트벡터머신을 실습해보겠습니다. (현재 UCI대학교의 피마인디언 당뇨병 데이터는링크가 삭제되었습니다. sklearn의 샘플데이터인 diabetes와는 전혀 다른 데이터입니다.)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "      0    1   2   3    4     5      6   7  8\n0     6  148  72  35    0  33.6  0.627  50  1\n1     1   85  66  29    0  26.6  0.351  31  0\n2     8  183  64   0    0  23.3  0.672  32  1\n3     1   89  66  23   94  28.1  0.167  21  0\n4     0  137  40  35  168  43.1  2.288  33  1\n..   ..  ...  ..  ..  ...   ...    ...  .. ..\n763  10  101  76  48  180  32.9  0.171  63  0\n764   2  122  70  27    0  36.8  0.340  27  0\n765   5  121  72  23  112  26.2  0.245  30  0\n766   1  126  60   0    0  30.1  0.349  47  1\n767   1   93  70  31    0  30.4  0.315  23  0\n\n[768 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>10</td>\n      <td>101</td>\n      <td>76</td>\n      <td>48</td>\n      <td>180</td>\n      <td>32.9</td>\n      <td>0.171</td>\n      <td>63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>2</td>\n      <td>122</td>\n      <td>70</td>\n      <td>27</td>\n      <td>0</td>\n      <td>36.8</td>\n      <td>0.340</td>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>5</td>\n      <td>121</td>\n      <td>72</td>\n      <td>23</td>\n      <td>112</td>\n      <td>26.2</td>\n      <td>0.245</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>1</td>\n      <td>126</td>\n      <td>60</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.1</td>\n      <td>0.349</td>\n      <td>47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>1</td>\n      <td>93</td>\n      <td>70</td>\n      <td>31</td>\n      <td>0</td>\n      <td>30.4</td>\n      <td>0.315</td>\n      <td>23</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>768 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./pima-indians-diabetes.csv\", header=None)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "총 768개의 샘플로 구성되어 있고,\n",
    "특성은 차례대로 임신횟수, 혈당농도(포도당 부하검사 2시간 후), 최저혈압(mm/Hg), 삼두근 피하지방 두께(mm), 혈청 인슐린 농도(포도당 부하검사 2시간 후), BMI(비만도 수치=몸무게kg/키m*2), 당뇨병 혈통 요인(가족력), 나이 및 당뇨병양성여부(1/0)입니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 서포트벡터 머신 실습(피마 인디언 당뇨병)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  # 서포트 벡터머신 분류기Classifier\n",
    "from sklearn.preprocessing import StandardScaler  # 모든 값을 정규화(평균0, 분산1)하기 위함"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "x = df.iloc[:, :8]  # 특성\n",
    "y = df.iloc[:, 8]  # 타겟"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# 데이터 쪼개기\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# 정규화\n",
    "std_scl = StandardScaler()\n",
    "std_scl.fit(x_train)\n",
    "x_train = std_scl.transform(x_train)\n",
    "x_test = std_scl.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련점수 : 0.82\n",
      "테스트점수 : 0.78\n"
     ]
    }
   ],
   "source": [
    "# 학습 및 테스트\n",
    "svc = SVC()\n",
    "svc.fit(x_train, y_train)\n",
    "\n",
    "print(f\"훈련점수 : {svc.score(x_train, y_train):.2f}\")\n",
    "print(f\"테스트점수 : {svc.score(x_test, y_test):.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 마치며\n",
    "\n",
    "이번 시간은 일반 머신러닝 알고리즘을 소개하는 마지막 시간으로, 주로 지도학습 위주로 설명을 드렸습니다. 이밖에도 비지도학습으로 군집화(클러스터링) 등 흥미로운 머신러닝 알고리즘들이 여러분의 자습에 맡깁니다.\n",
    "\n",
    "어려운 알고리즘이 무조건 좋은 것이 아니라 적합한 상황에 맞는 적당한 알고리즘을 경험적으로 선택하는 것이 더 중요하다고들 합니다. 저도 이에 동의하고요. 여기서 \"적당한\"이 굉장히 막연하게 들리실 수 있지만, 실제로 현업에서도 그렇게 \"경험적으로 적당히\" 진행합니다. 앞으로 배울 딥러닝에서도, 어느 정도 정확도가 적당한지, 어느 정도 개수의 유닛이나 레이어가 적당한지, 어떤 알고리즘이 적당한지 등등, 책만으로는 배울 수 없는 지점이 있습니다. 다행인 것은 깃헙이나 캐글 등 온라인으로 공개된 많은 데이터셋과 모델 예제가 있고, 구글이나 오픈AI 등 대기업에서도 이런 정보의 공유를 나름 적극적으로 지원해 주고 있다는 사실입니다.\n",
    "\n",
    "여러분이 처음 시도하는 머신러닝 모델도 아주 높은 확률로 처음에는 원하는 결과가 나오지 않을 것입니다. (이건 거의 확실합니다.) 그래서 하이퍼파라미터의 튜닝도 필요하고 모델 변경도 자주 해보고, 전이학습이나 데이터 강화 등의 다양한 기법으로 심폐소생술을 해야 합니다. 개인적인 취미라면 크게 문제되지 않지만, 6개월~1년 동안 인건비를 쏟아부어 만든 모델이라면 더더욱 퍼포먼스 튜닝이 중요하겠죠.\n",
    "\n",
    "다음 시간부터 딥러닝 기본 모델을 하나씩 살펴보면서 나름의 튜닝이나 최적화 방법도 같이 알아보도록 하겠습니다.\n",
    "\n",
    "오늘 수업은 여기까지입니다.\n",
    "수고하셨습니다!\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}