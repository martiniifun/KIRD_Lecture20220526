{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 딥러닝으로 다이빙하기 전에"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1-1. 우리가 지나쳤던 머신러닝 알고리즘들"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1-1-1. 의사결정나무 : \"올해 휴가는 어디로 갈까?\" since 1984, Leo Breiman\n",
    "\n",
    "\"이번 여름휴가는 바다로 갈까, 산으로 갈까?\"\n",
    "\n",
    "여름 휴가철이 되면 가족들과 함께 어디로 휴가 갈지 매번 고민들 하시죠?\n",
    "막연하긴 하지만, \"엄청 더우면(체감온도 30도 이상) 바다로 가고, 그렇지 않으면 산으로 간다.\"라고\n",
    "마음속으로 나름 확고한 알고리즘을 짜 둔 분이라면 큰 고민 없이 기준에 맞춰 의사결정을 하실 겁니다.\n",
    "\n",
    "의사결정트리 알고리즘은 이런 방식으로 특정 조건을 수립(조정)하고\n",
    "해당 조건에 따라 데이터를 분류하거나 예측합니다.\n",
    "\n",
    "# 휴가지 선정방법(예시)\n",
    "![휴가지선정방법](https://i.ibb.co/N1F6FHS/decisiontree-ex1.png)\n",
    "이처럼 분석과정이 직관적이고 이해하기 쉽다는 점이 의사결정나무의 가장 큰 장점입니다.\n",
    "\n",
    "# 의사결정나무의 장단점\n",
    "이밖에도 의사결정나무의 장단점으로는\n",
    "1. 알고리즘 해석이 용이하며, 분류기준이 되는 중요특성을 찾는 경우에도 유용하게 쓰인다.\n",
    "2. 정수, 실수데이터 뿐만 아니라 범주형데이터(성별 및 불리언) 또한 사용이 가능하다.\n",
    "3. 필요한 전처리가 비교적 적다. 스케일링, 정규화 등의 과정이 필요없어진다.\n",
    "4. 다만, 과적합되기 쉽다. 그래서 훈련데이터를 검토해서 이상값이나 예외값을 정제하는 등의 대책이 필요하다.\n",
    "\n",
    "# 의사결정나무 알고리즘(간략히)\n",
    "\n",
    "의사결정나무 알고리즘은 (다 비슷해 보이지만) 데이터를 분류하는 방법에 따라 제법 여러 가지로 나뉩니다.\n",
    "이 중 의사결정나무 알고리즘의 시초이며, 현재까지도 가장 많이 쓰이는 방법은, \"지니 계수\"와 \"분산 감소량\"을 사용해서 이진분류하는 방법입니다. 지니 계수는 범주형 변수를 분류할 때, 분산 감소량은 연속형 변수를 분류할 때 사용하는 방식입니다.\n",
    "\n",
    "> 지니계수는 원래 경제학에서 경제적 불평등(소득 불균형)을 계수화하는 방법입니다. 경제학자인 코라도지니Corrado Gini의 이름에서 딴 계수로, 완전 평등하다면 0, 완전불평등한 상태라면 1이 됩니다. 이를 머신러닝에 적용할 때에는 그 의미가 재해석되는데, 데이터의 속성들이 많이 일치하면 1이 되고, 반대로 속성들이 제각기 다른 형태이면 0이 됩니다. 우리가 원하는 것은 비슷한 데이터끼리 잘 묶어주는 것이므로, 분류 전후의 지니 계수의 차이가 큰 속성을 기준으로 우선분할합니다. 즉, 데이터들의 속성이 비슷한 것끼리 구분지어 묶이게 됩니다.\n",
    "\n",
    "위 방법을 가리켜 카트(CaRT, Classification and Regression Trees) 알고리즘이라고 줄여 부릅니다. 그 밖에도 지니계수 대신 정보이득(information gain)을 사용하는 ID3알고리즘, 이득비율(gain ration)을 사용하여 이진분류가 아니라 세 가지로 분류할 수 있는 C5.0알고리즘 등이 있습니다. 각 알고리즘이나 용어에 대해 궁금하신 분은 구글링을 해보시면 어마어마하게 많은 관련포스팅이 튀어나온다는 것에 꽤 놀라실 것입니다. 게다가 한글로요.. 공부하기 편한 시대입니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 의사결정나무를 만들어봅시다.\n",
    "\n",
    "## 사용데이터 : 붓꽃(Iris) 분류\n",
    "\n",
    "붓꽃(Iris)은 재미있는 특징이 있습니다. \"세토사\", \"버시컬러\", \"버지니카\" 세 개의 대표종이 (서로 다른 종임에도) 색깔과 형태가 굉장히 유사하다는 것입니다. 이걸 효과적으로 구분할 수 있는 거의 유일한 방법은 꽃잎의 길이와 너비, 그리고 꽃받침의 길이와 너비를 이용하는 것입니다.\n",
    "\n",
    "![](https://machinelearninghd.com/wp-content/uploads/2021/03/iris-dataset.png)\n",
    "\n",
    "과연 의사결정나무로도 효과적인 분류가 가능할까요?\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/56/Iris_dataset_scatterplot.svg/1280px-Iris_dataset_scatterplot.svg.png)\n",
    "where, sepal:꽃받침, petal:꽃잎"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "데이터셋 = datasets.load_iris()\n",
    "print(데이터셋['DESCR'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "특성데이터 = 데이터셋.data\n",
    "타겟데이터 = 데이터셋.target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "특성데이터  # 0~3 칼럼(특성)은 각각 sepal length, sepal width, petal length 및 petal width를 뜻합니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "타겟데이터  # 타겟데이터 0~2는 차례대로 \"세토사\", \"버시컬러\", \"버지니카\"입니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "본격적으로 모델을 구현하기 전에, 의사결정나무의 구조를 이미지로 시각화하기 위한 소프트웨어인\n",
    "그래프비즈(Graphviz)를 설치하겠습니다.\n",
    "![](https://i.ibb.co/h24w4V2/image.png)\n",
    "\n",
    "환경변수의 시스템변수 Path에 경로를 추가해야 하는데, 설치과정 중 환경변수에 추가할지 물어봅니다. 현재 사용자(current user)의 환경변수에 추가하겠다고 선택하고 설치를 마치시면 됩니다.\n",
    "\n",
    "# 붓꽃 데이터를 분류하기 위한 의사결정나무 구현 코드"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "꽃잎정보 = 특성데이터[:, 2:]  # 모든 데이터셋(행)을 사용하지만, 꽃잎길이와 꽃잎너비 두 개의 특성만 사용해보겠습니다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# 의사결정나무 모델의 클래스 생성\n",
    "분류기 = DecisionTreeClassifier(criterion='gini', max_depth=3)  # 지니계수gini 대신에 혼잡도entropy를 사용하기도 함."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier(max_depth=3)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 학습\n",
    "분류기.fit(꽃잎정보, 타겟데이터)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 학습이 완료된 결정나무 형태를 출력\n",
    "with open(\"./iris_dtree.dot\", 'w') as f:\n",
    "    tree.export_graphviz(분류기, out_file=f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "생성된 iris_dtree.dot 파일은\n",
    "익숙해지기 전까지는 알아보기 좀 어렵습니다.\n",
    "이렇게 생긴 텍스트 데이터입니다.\n",
    "\n",
    "```\n",
    "digraph Tree {\n",
    "node [shape=box] ;\n",
    "0 [label=\"X[1] <= 0.8\\ngini = 0.667\\nsamples = 150\\nvalue = [50, 50, 50]\"] ;\n",
    "1 [label=\"gini = 0.0\\nsamples = 50\\nvalue = [50, 0, 0]\"] ;\n",
    "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
    "2 [label=\"X[1] <= 1.75\\ngini = 0.5\\nsamples = 100\\nvalue = [0, 50, 50]\"] ;\n",
    "0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
    "3 [label=\"X[0] <= 4.95\\ngini = 0.168\\nsamples = 54\\nvalue = [0, 49, 5]\"] ;\n",
    "2 -> 3 ;\n",
    "4 [label=\"gini = 0.041\\nsamples = 48\\nvalue = [0, 47, 1]\"] ;\n",
    "3 -> 4 ;\n",
    "5 [label=\"gini = 0.444\\nsamples = 6\\nvalue = [0, 2, 4]\"] ;\n",
    "3 -> 5 ;\n",
    "6 [label=\"X[0] <= 4.85\\ngini = 0.043\\nsamples = 46\\nvalue = [0, 1, 45]\"] ;\n",
    "2 -> 6 ;\n",
    "7 [label=\"gini = 0.444\\nsamples = 3\\nvalue = [0, 1, 2]\"] ;\n",
    "6 -> 7 ;\n",
    "8 [label=\"gini = 0.0\\nsamples = 43\\nvalue = [0, 0, 43]\"] ;\n",
    "6 -> 8 ;\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'dot' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!dot -T png iris-dtree.dot -o iris-dtree.png"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}